{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising\n",
    "This is a implementation of post-reconstruction denosing using the exact network proposed by \n",
    "https://paperswithcode.com/paper/beyond-a-gaussian-denoiser-residual-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import liberies\n",
    "import os \n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# import cv2\n",
    "from PIL import Image\n",
    "\n",
    "#\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# about dataset \n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# \n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "\n",
    "# test metrics\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# import external liberaries\n",
    "import importlib\n",
    "import data_utils\n",
    "importlib.reload(data_utils)\n",
    "from data_utils import show_images_grid, show_error_profile, filterd_back_projection\n",
    "\n",
    "# set seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# from experiments.pytorchtools import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define DnCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define neural network\n",
    "class DnCNN(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, num_layers=17, num_features=64):\n",
    "        super(DnCNN, self).__init__()\n",
    "\n",
    "        layers = [\n",
    "            nn.Conv2d(in_channels, num_features, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "\n",
    "        for _ in range(num_layers-2):\n",
    "            layers.extend([\n",
    "                nn.Conv2d(num_features, num_features, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(num_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ])\n",
    "\n",
    "        layers.append(nn.Conv2d(num_features, out_channels, kernel_size=3, padding=1))\n",
    "\n",
    "        self.dncnn = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x - self.dncnn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define custome dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define custome dataset\n",
    "class WaterlooPairDataset(Dataset):\n",
    "    def __init__(self, clean_dir, noisy_dir, transform=None):\n",
    "        self.transform = transform\n",
    "        self.clean_dir = clean_dir\n",
    "        self.noisy_dir = noisy_dir\n",
    "        \n",
    "        # scan image file\n",
    "        clean_sinograms = sorted([f for f in os.listdir(self.clean_dir) if f.endswith(('.png', '.jpg', '.jpeg', '.bmp'))])\n",
    "        noisy_sinograms = sorted([f for f in os.listdir(self.noisy_dir) if f.endswith(('.png', '.jpg', '.jpeg', '.bmp'))])\n",
    "\n",
    "        assert len(clean_sinograms) == len(noisy_sinograms), \\\n",
    "            \"Number of clean sinograms and noisy sinograms should be equal\"\n",
    "        \n",
    "        self.sinogram_pairs = [(os.path.join(clean_dir, c), os.path.join(noisy_dir, n)) for c, n in zip(clean_sinograms, noisy_sinograms)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sinogram_pairs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        clean_path, noisy_path = self.sinogram_pairs[index]\n",
    "\n",
    "        # clean_sinogram = cv2.imread(clean_path, cv2.IMREAD_GRAYSCALE) \n",
    "        # noisy_sinogram = cv2.imread(noisy_path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        clean_sinogram = Image.open(clean_path).convert('L')\n",
    "        noisy_sinogram = Image.open(noisy_path).convert('L')\n",
    "        \n",
    "        ## apply reconstruction algorithm\n",
    "        clean_recon = filterd_back_projection(np.array(clean_sinogram))\n",
    "        noisy_recon = filterd_back_projection(np.array(noisy_sinogram))\n",
    "            \n",
    "        # Convert recon images to float tensors\n",
    "        #clean_recon = torch.from_numpy(clean_recon).float() / 255.0\n",
    "        #noisy_recon = torch.from_numpy(noisy_recon).float() / 255.0\n",
    "        \n",
    "        # Normalize the recon images\n",
    "        clean_recon = (clean_recon - np.min(clean_recon)) / (np.max(clean_recon) - np.min(clean_recon))\n",
    "        noisy_recon = (noisy_recon - np.min(noisy_recon)) / (np.max(noisy_recon) - np.min(noisy_recon))\n",
    "        \n",
    "        \n",
    "        if self.transform is not None:\n",
    "            clean_recon = self.transform(Image.fromarray(clean_recon))\n",
    "            noisy_recon = self.transform(Image.fromarray(noisy_recon))\n",
    "\n",
    "        return clean_recon, noisy_recon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(clean_dir, noisy_dir, transform=None, batch_size=32, num_workers=4):\n",
    "    dataset = WaterlooPairDataset(clean_dir, noisy_dir, transform)\n",
    "\n",
    "    # calculate dataset length\n",
    "    total_len = len(dataset)\n",
    "    train_len = int(0.8 * total_len)\n",
    "    val_len = int(0.1 * total_len)\n",
    "    test_len = total_len - train_len - val_len\n",
    "\n",
    "    # random_split dataset\n",
    "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_len, val_len, test_len])\n",
    "\n",
    "    # create dataloader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    \n",
    "    print(f\"train:{len(train_loader.dataset)}, val:{len(val_loader.dataset)}, test:{len(test_loader.dataset)}\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AddChannelDimension:\n",
    "#     def __call__(self, img):\n",
    "#         return np.expand_dims(img, axis=0)\n",
    "    \n",
    "\n",
    "# define data transform\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "\n",
    "# define input dir\n",
    "clean_dir = 'data/exploration_database_and_code/clean'\n",
    "noisy30_dir = 'data/exploration_database_and_code/noisy30'\n",
    "noisy25_dir = 'data/exploration_database_and_code/noisy25'\n",
    "noisy20_dir = 'data/exploration_database_and_code/noisy20'\n",
    "noisy15_dir = 'data/exploration_database_and_code/noisy15'\n",
    "noisy10_dir = 'data/exploration_database_and_code/noisy10'\n",
    "\n",
    "\n",
    "# define dataloader setting\n",
    "batch_size = 32\n",
    "num_workers = 12\n",
    "\n",
    "# create niosy30 datasetloader \n",
    "train_loader, val_loader, test_loader = create_dataloaders(clean_dir, noisy10_dir, transform=data_transform, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show training batch\n",
    "dataiter = iter(train_loader)\n",
    "\n",
    "batch_clean_tensor, batch_noisy_tensor = dataiter.next()\n",
    "batch_clean_recons, batch_noisy_recons = batch_clean_tensor.numpy(), batch_noisy_tensor.numpy()\n",
    "\n",
    "#\n",
    "print(f\"batch shape : {batch_clean_recons.shape}\")\n",
    "print(f\"feed data, range: {np.min(batch_clean_recons[0])} {np.max(batch_clean_recons[0])}\")\n",
    "\n",
    "#\n",
    "show_images_grid(np.squeeze(batch_clean_recons, axis=1), cmap='gray', figsize=(15, 15), suptitle='clean recon batch')\n",
    "show_images_grid(np.squeeze(batch_noisy_recons, axis=1), cmap='gray', figsize=(15, 15), suptitle='noisy recon batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure clean and noisy recon at the similar brightness\n",
    "idx = 0\n",
    "clean_recon = np.squeeze(batch_clean_recons, axis=1)[idx]\n",
    "noisy_recon = np.squeeze(batch_noisy_recons, axis=1)[idx]\n",
    "\n",
    "#\n",
    "print(f\"clean_recon, mean: {np.mean(clean_recon):.5f}\")\n",
    "print(f\"noisy_recon, mean: {np.mean(noisy_recon):.5f}\")\n",
    "\n",
    "# \n",
    "show_error_profile(clean_recon, noisy_recon, suptitle=\"clean_recon vs noisy_recon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traiing function\n",
    "def train(train_loader, model, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, (clean_img, noisy_img) in enumerate(train_loader):\n",
    "        clean_img, noisy_img = clean_img.to(device), noisy_img.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(noisy_img)\n",
    "        loss = criterion(outputs, clean_img)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "# validation function\n",
    "def validate(val_loader, model, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (clean_img, noisy_img) in enumerate(val_loader):\n",
    "            clean_img, noisy_img = clean_img.to(device), noisy_img.to(device)\n",
    "            outputs = model(noisy_img)\n",
    "            loss = criterion(outputs, clean_img)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters\n",
    "learning_rate = 0.01 # SGD\n",
    "momentum = 0.9 # SGD\n",
    "step_size = 10 # StepLR\n",
    "gamma = 0.1 # StepLR\n",
    "epochs = 50 # training epochs\n",
    "# patience = 10 # earlystop patience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = DnCNN().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "# early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary model\n",
    "summary(model, (1, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # writer = SummaryWriter(\"tf-logs/runs/DnCNN-noisy10-MSE-50epoch\")\n",
    "\n",
    "# # training loop\n",
    "# start_time = time.time()\n",
    "# for epoch in range(epochs):\n",
    "#     train_loss = train(train_loader, model, criterion, optimizer, device)\n",
    "#     val_loss = validate(val_loader, model, criterion, device)\n",
    "#     print(f'Epoch: {epoch+1}/{epochs}, Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')\n",
    "    \n",
    "#     # Log the losses to TensorBoard\n",
    "#     writer.add_scalars(\"Losses\", {\"Train\": train_loss, \"Val\": val_loss}, epoch)\n",
    "\n",
    "#     # early_stopping(val_loss, model)\n",
    "#     # if early_stopping.early_stop:\n",
    "#     #     print(\"Early stopping\")\n",
    "#     #     break\n",
    "\n",
    "#     scheduler.step()\n",
    "    \n",
    "    \n",
    "# end_time = time.time()\n",
    "# total_second = int(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model's weight\n",
    "# torch.save(model.state_dict(), 'checkpoints/DnCNN-noisy10-MSE-50epoch.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send result to wechat \n",
    "\n",
    "# import requests\n",
    "# headers = {\"Authorization\": \"eyJhbGciOiJFUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1aWQiOjM5MjI4LCJ1dWlkIjoiNTAyZTcyM2ItZjY2Mi00YTk4LWJkZmEtMzc1ZjdlOWM5NmFlIiwiaXNfYWRtaW4iOmZhbHNlLCJpc19zdXBlcl9hZG1pbiI6ZmFsc2UsInN1Yl9uYW1lIjoiIiwidGVuYW50IjoiYXV0b2RsIiwidXBrIjoiIn0.0IybMXdA-3z6KDYJDDGCj-_qqw6o4kya5usOFcLUtFL-ewBe35RnN8COQn4lO3umL-rWJ3er2PsIWZBjIl5XJw\"}\n",
    "# resp = requests.post(\"https://www.autodl.com/api/v1/wechat/message/send\",\n",
    "#                      json={\n",
    "#                          \"title\": \"训练结束\",\n",
    "#                          \"name\": \"DnCNN-noisy10-MSE-50epoch\",\n",
    "#                          \"content\": f\"training time: {total_second // 3600}h {(total_second % 3600) // 60}m\"\n",
    "#                      }, headers = headers)\n",
    "\n",
    "# print(resp.content.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define evaluating function\n",
    "def calculate_psnr(pred, target):\n",
    "    \"\"\"calculate PSNR between tensors\n",
    "    \"\"\"\n",
    "    pred = pred.squeeze(0).clamp(0, 1).cpu().numpy()\n",
    "    target = target.squeeze(0).clamp(0, 1).cpu().numpy()\n",
    "    return psnr(target, pred, data_range=1)\n",
    "\n",
    "\n",
    "def calculate_ssim(pred, target):\n",
    "    \"\"\"calculate SSIM between tensors\n",
    "    \"\"\"\n",
    "    pred = pred.squeeze(0).clamp(0, 1).cpu().numpy()\n",
    "    target = target.squeeze(0).clamp(0, 1).cpu().numpy()\n",
    "    return ssim(target, pred, data_range=1)\n",
    "\n",
    "def evaluate(model, test_loader, device):\n",
    "    \"\"\"evaluate from test_loaders\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    psnr_list = []\n",
    "    ssim_list = []\n",
    "    clean_imgs, noisy_imgs, denoised_imgs = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (clean_img, noisy_img) in enumerate(test_loader):\n",
    "            clean_img, noisy_img = clean_img.to(device), noisy_img.to(device)\n",
    "            denoised = model(noisy_img)\n",
    "\n",
    "            for i in range(clean_img.size(0)):\n",
    "                psnr_list.append(calculate_psnr(denoised[i], clean_img[i]))\n",
    "                ssim_list.append(calculate_ssim(denoised[i], clean_img[i]))\n",
    "\n",
    "                # Convert tensors to NumPy arrays and reshape to grayscale images\n",
    "                clean_np = clean_img[i].cpu().numpy().squeeze()\n",
    "                noisy_np = noisy_img[i].cpu().numpy().squeeze()\n",
    "                denoised_np = denoised[i].cpu().numpy().squeeze()\n",
    "\n",
    "                # Append the grayscale images as a tuple to the results list\n",
    "                clean_imgs.append(clean_np)\n",
    "                noisy_imgs.append(noisy_np)\n",
    "                denoised_imgs.append(denoised_np)\n",
    "                \n",
    "\n",
    "    avg_psnr = np.mean(psnr_list)\n",
    "    avg_ssim = np.mean(ssim_list)\n",
    "\n",
    "    return avg_psnr, avg_ssim, (clean_imgs, noisy_imgs, denoised_imgs)\n",
    "\n",
    "def get_evaluation_results(model, test_loaders, device):\n",
    "    \"get evulation result\"\n",
    "    avg_psnr_results = []\n",
    "    avg_ssim_results = []\n",
    "    clean_imgs_results, noisy_imgs_results, denoised_imgs_results = [], [], [] \n",
    "\n",
    "    for _, test_loader in test_loaders.items():\n",
    "        avg_psnr, avg_ssim, (clean_imgs, noisy_imgs, denoised_imgs) = evaluate(model, test_loader, device)\n",
    "        avg_psnr_results.append(avg_psnr)\n",
    "        avg_ssim_results.append(avg_ssim)\n",
    "        clean_imgs_results.append(clean_imgs)\n",
    "        noisy_imgs_results.append(noisy_imgs)\n",
    "        denoised_imgs_results.append(denoised_imgs)\n",
    "\n",
    "    clean_imgs_results, noisy_imgs_results, denoised_imgs_results = np.array(clean_imgs_results), np.array(noisy_imgs_results), np.array(denoised_imgs_results)\n",
    "    \n",
    "    return avg_psnr_results, avg_ssim_results, clean_imgs_results, noisy_imgs_results, denoised_imgs_results\n",
    "\n",
    "def plot_evaluation_results(x, avg_psnr_results, avg_ssim_results, title=\"PSNR & SSIM plot\", save_path=None):\n",
    "    \"\"\"plot evaluation results\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 5))\n",
    "    axs[0].plot(x, avg_psnr_results, color='blue')\n",
    "    axs[0].set_ylabel(\"PSNR(db)\")\n",
    "    axs[1].plot(x, avg_ssim_results, color='red')\n",
    "    axs[1].set_ylabel(\"SSIM(%)\")\n",
    "\n",
    "    for i, j in zip(x, avg_psnr_results):\n",
    "        axs[0].annotate(f\"{j:.4f}\", xy=(i, j), xycoords='data', xytext=(0, 10),\n",
    "                        textcoords='offset points', ha='center', va='bottom')\n",
    "\n",
    "    for i, j in zip(x, avg_ssim_results):\n",
    "        axs[1].annotate(f\"{j:.4f}\", xy=(i, j), xycoords='data', xytext=(0, 10),\n",
    "                        textcoords='offset points', ha='center', va='bottom')\n",
    "\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    fig.suptitle(title)\n",
    "    \n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_noise_level_images(x, clean_imgs_results, noisy_imgs_results, denoised_imgs_results, idx=0):\n",
    "    \"\"\"plot_noise_level_images\n",
    "    \"\"\"\n",
    "    num_noise_levels = clean_imgs_results.shape[0]\n",
    "\n",
    "    # Create a grid of subplots with num_noise_levels rows and 3 columns for clean, noisy, and denoised images\n",
    "    fig, axs = plt.subplots(num_noise_levels, 3, figsize=(20, num_noise_levels * 5))\n",
    "\n",
    "    for i in range(num_noise_levels):\n",
    "        # Display clean image\n",
    "        axs[i, 0].imshow(clean_imgs_results[i, idx], cmap='gray')\n",
    "        axs[i, 0].set_title(f\"Clean ({x[i]})\")\n",
    "\n",
    "        # Display noisy image\n",
    "        axs[i, 1].imshow(noisy_imgs_results[i, idx], cmap='gray')\n",
    "        axs[i, 1].set_title(f\"Noisy ({x[i]})\")\n",
    "\n",
    "        # Display denoised image\n",
    "        axs[i, 2].imshow(denoised_imgs_results[i, idx], cmap='gray')\n",
    "        axs[i, 2].set_title(f\"Denoised ({x[i]})\")\n",
    "\n",
    "        # Remove axis ticks and labels\n",
    "        axs[i, 0].set_xticks([])\n",
    "        axs[i, 0].set_yticks([])\n",
    "        axs[i, 1].set_xticks([])\n",
    "        axs[i, 1].set_yticks([])\n",
    "        axs[i, 2].set_xticks([])\n",
    "        axs[i, 2].set_yticks([])\n",
    "\n",
    "    # Adjust the spacing between subplots\n",
    "    fig.subplots_adjust(hspace=0.2, wspace=0.3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model from cheack point\n",
    "# np.random.seed(10)\n",
    "# torch.manual_seed(10)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = DnCNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test dataloaders \n",
    "test_loaders = {\n",
    "    \"noisy30\": create_dataloaders(clean_dir, noisy30_dir, transform=data_transform, batch_size=batch_size, num_workers=num_workers)[-1],\n",
    "    \"noisy25\": create_dataloaders(clean_dir, noisy25_dir, transform=data_transform, batch_size=batch_size, num_workers=num_workers)[-1],\n",
    "    \"noisy20\": create_dataloaders(clean_dir, noisy20_dir, transform=data_transform, batch_size=batch_size, num_workers=num_workers)[-1],\n",
    "    \"noisy15\": create_dataloaders(clean_dir, noisy15_dir, transform=data_transform, batch_size=batch_size, num_workers=num_workers)[-1],\n",
    "    \"noisy10\": create_dataloaders(clean_dir, noisy10_dir, transform=data_transform, batch_size=batch_size, num_workers=num_workers)[-1]\n",
    "}\n",
    "\n",
    "x = list(test_loaders.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. evaluate result that train on `noisy30` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained network weight\n",
    "model.load_state_dict(torch.load('checkpoints/DnCNN-noisy30-MSE-50epoch.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate trained noisy30 network on differnt noise level testset\n",
    "avg_psnr_results, avg_ssim_results, clean_imgs_results, noisy_imgs_results, denoised_imgs_results = get_evaluation_results(model, test_loaders, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot PSNR and SSIM\n",
    "plot_evaluation_results(x, avg_psnr_results, avg_ssim_results, title=\"PSNR & SSIM plot(noisy30)\", save_path=\"media/PSNR & SSIM plot(noisy30).png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize clean, noisy, and denoised image \n",
    "plot_noise_level_images(x, clean_imgs_results, noisy_imgs_results, denoised_imgs_results, idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. evaluate result that train on `noisy25` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained network weight\n",
    "model.load_state_dict(torch.load('checkpoints/DnCNN-noisy25-MSE-50epoch.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate trained noisy25 network on differnt noise level testset\n",
    "avg_psnr_results, avg_ssim_results, clean_imgs_results, noisy_imgs_results, denoised_imgs_results = get_evaluation_results(model, test_loaders, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot PSNR and SSIM\n",
    "plot_evaluation_results(x, avg_psnr_results, avg_ssim_results, title=\"PSNR & SSIM plot(noisy25)\", save_path=\"media/PSNR & SSIM plot(noisy25).png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize clean, noisy, and denoised image \n",
    "plot_noise_level_images(x, clean_imgs_results, noisy_imgs_results, denoised_imgs_results, idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. evaluate result that train on `noisy20` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained network weight\n",
    "model.load_state_dict(torch.load('checkpoints/DnCNN-noisy20-MSE-50epoch.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate trained noisy20 network on differnt noise level testset\n",
    "avg_psnr_results, avg_ssim_results, clean_imgs_results, noisy_imgs_results, denoised_imgs_results = get_evaluation_results(model, test_loaders, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot PSNR and SSIM\n",
    "plot_evaluation_results(x, avg_psnr_results, avg_ssim_results, title=\"PSNR & SSIM plot(noisy20)\", save_path=\"media/PSNR & SSIM plot(noisy20).png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize clean, noisy, and denoised image \n",
    "plot_noise_level_images(x, clean_imgs_results, noisy_imgs_results, denoised_imgs_results, idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. evaluate result that train on `noisy15` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained network weight\n",
    "model.load_state_dict(torch.load('checkpoints/DnCNN-noisy15-MSE-50epoch.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate trained noisy15 network on differnt noise level testset\n",
    "avg_psnr_results, avg_ssim_results, clean_imgs_results, noisy_imgs_results, denoised_imgs_results = get_evaluation_results(model, test_loaders, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot PSNR and SSIM\n",
    "plot_evaluation_results(x, avg_psnr_results, avg_ssim_results, title=\"PSNR & SSIM plot(noisy15)\", save_path=\"media/PSNR & SSIM plot(noisy15).png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize clean, noisy, and denoised image \n",
    "plot_noise_level_images(x, clean_imgs_results, noisy_imgs_results, denoised_imgs_results, idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. evaluate result that train on `noisy10` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained network weight\n",
    "model.load_state_dict(torch.load('checkpoints/DnCNN-noisy15-MSE-50epoch.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate trained noisy10 network on differnt noise level testset\n",
    "avg_psnr_results, avg_ssim_results, clean_imgs_results, noisy_imgs_results, denoised_imgs_results = get_evaluation_results(model, test_loaders, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot PSNR and SSIM\n",
    "plot_evaluation_results(x, avg_psnr_results, avg_ssim_results, title=\"PSNR & SSIM plot(noisy10)\", save_path=\"media/PSNR & SSIM plot(noisy10).png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize clean, noisy, and denoised image \n",
    "plot_noise_level_images(x, clean_imgs_results, noisy_imgs_results, denoised_imgs_results, idx=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
