{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising\n",
    "https://paperswithcode.com/paper/beyond-a-gaussian-denoiser-residual-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import liberies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "\n",
    "from skimage.transform import radon, iradon\n",
    "from skimage.metrics import peak_signal_noise_ratio\n",
    "\n",
    "\n",
    "from data_utils import filterd_back_projection, split_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define DnCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define neural network\n",
    "class DnCNN(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, num_layers=17, num_features=64):\n",
    "        super(DnCNN, self).__init__()\n",
    "\n",
    "        layers = [\n",
    "            nn.Conv2d(in_channels, num_features, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "\n",
    "        for _ in range(num_layers-2):\n",
    "            layers.extend([\n",
    "                nn.Conv2d(num_features, num_features, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(num_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ])\n",
    "\n",
    "        layers.append(nn.Conv2d(num_features, out_channels, kernel_size=3, padding=1))\n",
    "\n",
    "        self.dncnn = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x - self.dncnn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define dataset, dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define custome dataset\n",
    "class WaterlooPairDataset(Dataset):\n",
    "    def __init__(self, clean_dir, noisy_dir, transform=None):\n",
    "        self.clean_dir = clean_dir\n",
    "        self.noisy_dir = noisy_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.clean_files = os.listdir(clean_dir)\n",
    "        self.noisy_files = os.listdir(noisy_dir)\n",
    "\n",
    "        self.clean_files.sort()\n",
    "        self.noisy_files.sort()\n",
    "\n",
    "        assert len(self.clean_files) == len(self.noisy_files), \\\n",
    "            \"Number of clean files and noisy files should be equal\"\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.clean_files)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        clean_path = os.path.join(self.clean_dir, self.clean_files[index])\n",
    "        noisy_path = os.path.join(self.noisy_dir, self.noisy_files[index])\n",
    "\n",
    "        clean_img = cv2.imread(clean_path, 0)\n",
    "        noisy_img = cv2.imread(noisy_path, 0)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            clean_img = self.transform(clean_img)\n",
    "            noisy_img = self.transform(noisy_img)\n",
    "        \n",
    "        return clean_sinogram, noisy_sinogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset : 3795\n",
      "val_dataset : 474\n",
      "test_dataset : 475\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "clean_dir = 'data/exploration_database_and_code/clean'\n",
    "noisy30_dir = 'data/exploration_database_and_code/noisy30'\n",
    "noisy15_dir = 'data/exploration_database_and_code/noisy15'\n",
    "noisy10_dir = 'data/exploration_database_and_code/noisy10'\n",
    "\n",
    "\n",
    "# define data transform\n",
    "data_transfrom = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# create dataset\n",
    "noisy30_pair_dataset = WaterlooPairDataset(clean_dir, noisy30_dir, transforms=data_transfrom)\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = split_dataset(noisy30_pair_dataset)\n",
    "\n",
    "print(f\"train_dataset {len(train_dataset)}\")\n",
    "print(f\"val_dataset {len(val_dataset)}\")\n",
    "print(f\"test_dataset {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_denoising_datasets(clean_dir, noisy_dir, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n",
    "    # read local images\n",
    "    image_paths = [os.path.join(root_path, img_name) for img_name in os.listdir(root_path)]\n",
    "    random.shuffle(image_paths)\n",
    "    \n",
    "    total_images = len(image_paths)\n",
    "    train_size = int(total_images * train_ratio)\n",
    "    \n",
    "    val_size = int(total_images * val_ratio)\n",
    "    test_size = total_images - train_size - val_size\n",
    "    \n",
    "    # define data transform\n",
    "    data_transfrom = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "    ])\n",
    "    \n",
    "    # create dataset with degisned poisson noise\n",
    "    dataset = PoissonNoisyDataset(image_paths, lam=5, scale=1, transform=data_transfrom)\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "data_root = \"data/exploration_database_and_code/pristine_images/\"\n",
    "train_dataset, val_dataset, test_dataset = create_denoising_datasets(data_root)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
